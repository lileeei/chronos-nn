### 项目迭代计划：从零实现现代RNN

---

#### **迭代 0: 项目初始化与基础架构 (准备阶段)**

**目标:** 搭建项目骨架，实现核心的数学工具，为后续的神经网络开发奠定基础。

*   **任务 1: 环境搭建**
    *   初始化 Git 仓库。
    *   配置 `Cargo.toml`，添加核心依赖:
        *   `ndarray`: 用于高效的矩阵和多维数组运算。
        *   `rand`: 用于权重初始化。
        *   `serde`: (可选，未来用于模型序列化)。
    *   建立模块化的项目结构 (例如 `src/layers`, `src/optimizers`, `src/activations`)。

*   **任务 2: 基础数学库**
    *   基于 `ndarray` 封装常用的矩阵运算（乘法、加法、元素级操作）。
    *   实现第一批激活函数 (`tanh`, `sigmoid`) 及其导数。
    *   实现均方误差（MSE）损失函数。

*   **任务 3: 单元测试**
    *   为所有数学和激活函数编写单元测试，确保计算准确性。

---

#### **迭代 1: 实现 Vanilla RNN (阶段一)**

**目标:** 构建并训练一个最基础的循环神经网络。

*   **任务 1: RNN Cell 实现**
    *   根据公式 `h_t = tanh(W_hh * h_{t-1} + W_ih * x_t + b_h)` 实现一个简单的 RNN 单元（Cell）。
    *   实现完整的前向传播逻辑，能够处理整个序列输入。

*   **任务 2: 反向传播 (BPTT)**
    *   为 Vanilla RNN 实现随时间反向传播（BPTT）算法。
    *   实现一个简单的梯度下降优化器来更新权重。

*   **任务 3: 端到端测试**
    *   创建一个测试用例，例如使用 RNN 预测正弦波的下一个点。
    *   将整个训练和预测流程跑通。

---

#### **迭代 2: 功能增强与优化 (阶段二)**

**目标:** 解决基础 RNN 的常见问题，并提升模型的灵活性和性能。

*   **任务 1: 梯度裁剪**
    *   实现梯度裁剪（Gradient Clipping）逻辑，防止梯度爆炸。

*   **任务 2: 架构模块化**
    *   将激活函数抽象为 Trait，方便后续添加 `ReLU` 等新函数。
    *   实现对批处理（Batch Processing）的支持，以提升训练效率。

*   **任务 3: 扩展模型应用**
    *   支持 `Many-to-one` 结构，用于处理分类任务。
    *   使用字符级语言模型作为新的测试用例。

---

#### **迭代 3: 实现 LSTM (阶段三)**

**目标:** 实现长短期记忆网络（LSTM），以解决长期依赖问题。

*   **任务 1: LSTM Cell 实现**
    *   根据标准公式实现 LSTM 单元，包括遗忘门、输入门和输出门。
    *   重点关注细胞状态 `C_t` 的正确更新。

*   **任务 2: LSTM 层集成**
    *   将 LSTM Cell 封装成一个完整的层，处理序列数据。
    *   实现 LSTM 的前向和反向传播。

*   **任务 3: 测试与验证**
    *   设计一个需要长距离依赖的测试任务（例如，`"A...B...C -> D"` 的序列模式）。
    *   在语言建模等任务上与 Vanilla RNN 进行性能对比。

---

#### **迭代 4: 实现 GRU (阶段四)**

**目标:** 实现门控循环单元（GRU），作为 LSTM 的一个更高效的替代方案。

*   **任务 1: GRU Cell 实现**
    *   根据公式实现 GRU 单元，包括重置门和更新门。

*   **任务 2: GRU 层集成**
    *   将 GRU Cell 封装成一个层，并实现其前向和反向传播。

*   **任务 3: 性能对比**
    *   在与 LSTM 相同的任务上进行测试。
    *   记录并比较 GRU 和 LSTM 的训练速度、收敛情况和最终性能。

---

#### **后续迭代计划 (阶段五至七)**

*   **迭代 5: 双向 RNN (Bi-RNN)**
    *   **目标:** 让模型能同时利用过去和未来的上下文信息。
    *   **实现:** 创建一个通用的双向层封装，可包裹任何 RNN（Vanilla, LSTM, GRU）。

*   **迭代 6: 深度 RNN**
    *   **目标:** 通过堆叠多层 RNN 来增加模型深度和容量。
    *   **实现:** 支持多层 RNN 网络的构建，并引入 Dropout 和残差连接来优化训练。

*   **迭代 7: 注意力机制入门**
    *   **目标:** 为迈向 Transformer 架构做准备。
    *   **实现:** 构建一个基础的 Encoder-Decoder 模型，并为其增加简单的注意力（Attention）机制。

### 贯穿所有迭代的原则 (Rust 特定)

*   **内存效率:** 始终关注内存分配，避免不必要的 `clone`，充分利用 `ndarray` 的视图（View）和原地修改（in-place）操作。
*   **模块化设计:** 积极使用 Trait 来定义通用接口（如 `Layer`, `Optimizer`, `Activation`），使代码可扩展、可组合。
*   **并行计算:** 在支持批处理后，研究使用 `rayon` 来并行处理一个批次中的多个样本，加速计算。
*   **持续测试:** 每个新功能都必须有对应的单元测试和集成测试。 